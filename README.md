# homework3
## Lovtsov Vladimir

Чтобы собрать docker контейнер:
  $ cd images/airflow-ml-base
  $ docker build -t airflow-ml-base:latest .

Настройка переменных окружения:
  $ export FERNET_KEY=$(python -c "from cryptography.fernet import Fernet; FERNET_KEY = Fernet.generate_key().decode(); print(FERNET_KEY)")

Также необходимо добавить в airflow variables /data/models/{{ds}}/model.pkl

Для запуска airflow из корня:
  $ docker compose up --build
  
  
  Легенда:
1) Откуда-то берутся данные... Мы их используем для обучения МЛ модельки для задачи классификации.
2) Еженедельно, мы переобучаем модель на новых данных, ручками смотрим на метрики и если класс, то выкатываем ее на прод.
3) Ежедневно, текущая выбранная нами модель скорит данные и записывает предсказания куда-то.
4) Эти предсказания используют -- все счастливы =)

В ДЗ предлагается на основе `airflow` реализовать описанную выше схему, к деталям:

**Основная часть:**

0) Поднимите airflow локально, используя `docker compose` (можно использовать из примера https://github.com/made-ml-in-prod-2021/airflow-examples/)
1) Реализуйте dag, который генерирует данные для обучения модели (генерируйте данные -- можете использовать как генератор синтетики из первой дз, так и что-то из датасетов sklearn). Вам важно проэмулировать ситуации постоянно поступающих данных (5 баллов)

    - записывайте данные в `/data/raw/{{ ds }}/data.csv` и `/data/raw/{{ ds }}/target.csv`

2) Реализуйте dag, который обучает модель еженедельно, используя данные за текущий день. В вашем пайплайне должно быть как минимум 4 стадии, но дайте волю своей фантазии =) (10 баллов)

    - подготовить данные для обучения (например, считать из `/data/raw/{{ ds }}` и положить `/data/processed/{{ ds }}/train_data.csv`)
    - расплитить их на train/val
    - обучить модель на train, сохранить в `/data/models/{{ ds }}`
    - провалидировать модель на val (сохранить метрики к модельке)

3) Реализуйте dag, который использует модель ежедневно (5 баллов)
    - принимает на вход данные из пункта 1 (`data.csv`)
    - считывает путь до модельки из airflow variables (идея в том, что когда нам нравится другая модель и мы хотим ее на прод)
    - делает предсказание и записывает их в `/data/predictions/{{ ds }}/predictions.csv`

4) Вы можете выбрать 2 пути для выполнения ДЗ:
    - поставить все необходимые пакеты в образ с `airflow` и использовать `BashOperator`, `PythonOperator` (1 балл)
    - использовать `DockerOperator` -- тогда выполнение каждой из тасок должно запускаться в собственном контейнере  
      * один из дагов реализован с помощью `DockerOperator` (5 баллов)
      * все даги реализованы только с помощью `DockerOperator` (пример https://github.com/made-ml-in-prod-2021/airflow-examples/blob/main/dags/11_docker.py). По технике, вы можете использовать такую же структуру как в примере, пакуя в разные докеры скрипты, можете использовать общий докер с вашим пакетом, но с разными точками входа для разных тасок. Прикольно, если вы покажете, что для разных тасок можно использовать разный набор зависимостей (10 баллов)

      https://github.com/made-ml-in-prod-2021/airflow-examples/blob/main/dags/11_docker.py#L27 в этом месте пробрасывается путь с хостовой машины, используйте здесь путь типа `/tmp` или считывайте из переменных окружения.

5) Традиционно, самооценка (1 балл)
